---
layout: default
title: Least Angle Regression
tags: [Machine Learning]
---

<h2>背景知识</h2>
<p>
最小角回归和模型选择比较像，是一个逐步的过程，每一步都选择一个相关性最大的特征，总的运算步数只和特征的数目有关，和训练集的大小无关。最小角回归训练时的输入为特征矩阵 $$$ X = \{X_1, X_2,..., X_P\} $$$，和期输出向量$$$ Y = \{y_1, y_2,..., y_N \} $$$，$$$X_i$$$ 是长度为$$$N$$$的矩阵，$$$N$$$表示训练集的大小，$$$P$$$则是特征的数目。还有一点需要注意的是，向量$$$X_i$$$ 和 $$$Y$$$ 都是正则化之后的向量，即它们的元素的均值为0，且每个向量的长度都是1，这样做的目的是为了后面计算相关性以及角度的方便。
</p>

<h3>相关性</h3>
<p>
相关性一般是用来衡量两个向量之间的相关程度，通常采用相关性公式进行计算，其中A，B为向量：
</p>

$$ \begin{eqnarray*}
	corr &=& \frac{Cov(A,B)}{\sqrt{Var(A)Var(B)}} \\
		 &=& \frac{E[(A-\bar{A})(B-\bar{B})]}{\sqrt{E[(A-\bar{A})^2]E[(B-\bar{B})^2]}}
\end{eqnarray*} $$

<p>
corr的绝对值越大，表示A，B的相关性越大，反之则越小。corr的符号则表示这两个向量是正相关还是负相关。由于最小回归的训练数据是经过正则化的，即$$$ \bar{A} = \bar{B} = 0 $$$，$$$|A| = |B| = 1$$$，所以上面的计算公式可以进行简化。
</p>

$$ \begin{eqnarray*}
	corr &=& \frac{E(AB)}{\sqrt{E(A^2)E(B^2)}} \\
		 &=& \frac{E(AB)}{\sqrt{1*1}} \\
		 &=& E(AB)
\end{eqnarray*} $$

<p>
在最小角回归中，$$$E(AB) = \frac{AB}{n} $$$，其中n是向量A或B中元素的个数，因而n是定值，所以为了方便计算，在计算向量的相关性时，我们只需要计算$$$ AB $$$即可。为了计算每个特征和期望输出的相关性，我们需要计算每个$$$X_i$$$ 与 $$$Y$$$的相关性，这种情况下，我们可以采用矩阵和向量的乘法来进行运算，这里X是$$$N \times P$$$的特征矩阵，Y则是大小为N的期望输出向量。
</p>

$$ \begin{eqnarray*}
	Corr &=& X^T Y
\end{eqnarray*} $$

<p>
这里的$$$Corr$$$是一个大小为P的向量，且 $$$Corr[i]$$$表示特征$$$X_i$$$与$$$Y$$$的相关性。
</p>

<h2>参数选择过程</h2>
<p>
在最小角回归中，参数选择的准则就是相关性，每次都选择和期望输出相关性最大的特征，即$$$max(|Corr[i]|)$$$。
</p>

<h3>前进方向</h3>
<p>
在我们的特征矩阵中，共有P个特征向量，每个向量都是N维的，期望输出矩阵Y也是N维的，因此我们可以将这些特征向量以及期望输出向量看作是N维空间中的点，而我们的目的就是在这个空间中找到一条从原点到Y的路径。需要满足的条件就是这条路径可以使用特征矩阵的线性组来合表示，而这些特征矩阵的系数就是线性回归的权重向量。
</p>

<p>
$$$ \hat{u}_A$$$ 表示从原点出发的一条路径，其初始值为$$$ \vec{0}$$$，我们的目的是在经过若干步之后，使得$$$ \hat{u}_A = Y $$$。每一步中，$$$\hat{u}_A$$$的增量为 $$$\hat{\gamma}u_A$$$，其中$$$u_A$$$ 为单位向量，它指明了 $$$\hat{u}_A$$$ 的前进方向，$$$\hat{\gamma}$$$ 则是前进的距离。
</p>
