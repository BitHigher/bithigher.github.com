---
layout: post
title: PCA:Principal Component Analysis
tags: [Machine Learning, Math]
categories: [Machine Learning]
keywords:[PCA,特征,feature,矩阵分解,Matrix]
---

<p>
$$$ X $$$ is a $$$ n \times p $$$ matrix, where n is the number of objects and p is the number of features. $$$X$$$ is zero mean.
</p>

<p>
$$
	X^T X = nV
$$
$$$ V $$$ is the covariance matrix of $$$X$$$.
</p>

<p>
In PCA, we want to project the original data into a lower space, suppose we want to project the p-dimensional data into line space, and the unit vector of the line is $$$ \vec{w} $$$, then for $$$ \vec{x}_i $$$ the projection is $$$ \vec{x}_i \cdot \vec{w} $$$, and the coordinate in p-dimensional space is $$$ (\vec{x}_i \cdot \vec{w}) \vec{w} $$$. The residual is:
</p>

<p>
$$ \begin{eqnarray*}
	\|\vec{x}_i - (\vec{x}_i \cdot \vec{w}) \vec{w}\|^2
	&=& \|\vec{x}_i\|^2-2(\vec{x}_i\cdot\vec{w})^2 + \|(\vec{x}_i \cdot \vec{w})\vec{w}\|^2 \\
	&=& \|\vec{x}_i\|^2 - (\vec{x}_i \cdot \vec{w})^2

\end{eqnarray*} $$
</p>

<p>
For all vectors, the residuals is:
</p>

<p>
$$ \begin{eqnarray*}
	RSS(\vec{w}) &=& \sum_{i=1}^n\|\vec{x}_i\|^2 - \sum_{i=1}^n(\vec{x}_i \cdot \vec{w})^2 \\
\end{eqnarray*} $$
</p>

<p>
Because $$$\| \vec{x}_i \|^2$$$ does not depend on \vec{w}, to minimize RSS, we need to maximize $$$ \sum_{i=1}^n(\vec{x}_i \cdot \vec{w})^2 $$$. Because $$$ X$$$ is zero mean， so
</p>

<p>
$$ \begin{eqnarray*}
	E(X\vec{w}) &=& \frac{1}{n}\sum_{i=1}^n \vec{x}_i\vec{w}	\\
				&=& (\frac{1}{n}\sum_{i=1}^n \vec{x}_i) \vec{w} \\
				&=& E(X)\vec{w} \\
				&=& 0
\end{eqnarray*} $$


$$ \begin{eqnarray*}
	\sum_{i=1}^n(\vec{x}_i \cdot \vec{w})^2 
				&=& \sum_{i=1}^n(\vec{x}_i \cdot \vec{w} - 0)^2 \\
				&=& \sum_{i=1}^n(\vec{x}_i\cdot\vec{w} - E(X\vec{w}))^2 \\
				&=& n\frac{1}{n}\sum_{i=1}^n(\vec{x}_i\cdot\vec{w} - E(X\vec{w}))^2 \\
				&=& n Var(X\vec{w})

\end{eqnarray*} $$
</p>

<p>
Now we can transform RSS into:
</p>

<p>
$$ \begin{eqnarray*}
	RSS(\vec{w}) &=& \sum_{i=1}^n \|\vec{x}_i\|^2 - \sum_{i=1}^n(\vec{x}_i \cdot \vec{w})^2 \\
				 &=& \sum_{i=1}^n \|\vec{x}_i\|^2 - n Var(X\vec{w})
\end{eqnarray*} $$
</p>

<p>
So, in order to minimize RSS, we just need to maximize variance $$$Var(X\vec{w})$$$.
</p>

<p>
$$ \begin{eqnarray*}
	\sigma_{\vec{w}}^2 &=& \frac{1}{n}\sum_{i=1}^n(\vec{x}_i\cdot\vec{w})^2 \\
	                   &=& \frac{1}{n}(X\vec{w})^T(X\vec{w})  \\
					   &=& \frac{1}{n}\vec{w}^TX^TX\vec{w} \\
					   &=& \vec{w}^T \frac{X^TX}{n} \vec{w} \\
					   &=& \vec{w}^T V \vec{w} 
\end{eqnarray*} $$
</p>

<p>
What we want to do is to find a unit vector $$$\vec{w}$$$, so that $$$\sigma_{\vec{w}}^2$$$ can be maximized.
</p>
